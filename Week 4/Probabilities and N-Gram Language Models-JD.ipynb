{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Message Spam Detection\n",
    "\n",
    "\n",
    "## The Business Use Case\n",
    "\n",
    "You are the CEO of a new email service company trying to attract capital for your next growth stage. Most of the private equity firms you have spoken to want to see a user base of at least 100,000 users prior to commiting. Due to your superb marketing team, you estimate that each day you attract 1000 new users. Typically, 10% of all email messages are spam, and an average user receives 50 emails per day.\n",
    "\n",
    "One of your data scientists says he can provide you a model with **90% accuracy** in classifying spam / ham. Another data scientist says she can build a model that is only **80% accurate** but has **100% recall**. A third data scientist says he can build a model that has **80% accuracy** with **100% precision**.\n",
    "\n",
    "Which of the above models would you pick to model? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T01:53:33.042415Z",
     "start_time": "2021-04-07T01:53:33.011235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1037, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"spam-sms.csv\", encoding='latin-1')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T01:53:35.072675Z",
     "start_time": "2021-04-07T01:53:35.055285Z"
    }
   },
   "outputs": [],
   "source": [
    "data=data.rename(columns={\"ï»¿class\": \"class\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T01:53:42.702740Z",
     "start_time": "2021-04-07T01:53:42.698952Z"
    }
   },
   "outputs": [],
   "source": [
    "Y = data[\"class\"].values\n",
    "X = data[\"text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Rule\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "For our purposes, we will redefine this as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P(spam|text) = \\frac{P(text|spam)P(spam)}{P(text)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here,\n",
    "\n",
    "- **prior** means before seeing any new text (evidence). Our impression of the likelihood of certain words appearing before new evidence is introduced.\n",
    "- **text** is the new message (evidence) being introduced that we want to classify as either spam or ham. Let's say we have a new text message `When are you coming home? I'm hungry.`.\n",
    "\n",
    "- $P(text)$ is the **prior likelihood** of seeing a particular text message with that exact combination of words. For instance, `P(\"the car is\")` will be significantly higher than `P(\"Downstream supply chain agents\")`, especially in the **context of text messages**.\n",
    "- $P(spam)$ is the likelihood that any text message will be spam. This is computed in our dataset:\n",
    "```python\n",
    "p_spam = sum(data[\"class\"] == \"spam\") / len(data)\n",
    "p_ham = 1 - p_spam # since there are only two classes\n",
    "```\n",
    "- $P(text|spam)$ is our **likelihood**. More specifically, the likelihood of this text message given that it is a piece of spam. It is saying, *let's assume that this message is spam. Knowing that, how likely is it that we'll find this particular combination of words in the text message?*\n",
    "\n",
    "In order to quickly get our likelihoods, we'll need to create a **likelihood table**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T01:53:46.845150Z",
     "start_time": "2021-04-07T01:53:46.830373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the spam_data is (156, 2). The shape of the ham_data is (881, 2).\n"
     ]
    }
   ],
   "source": [
    "spam_data = data[data[\"class\"] == \"spam\"]\n",
    "ham_data = data[data[\"class\"] == \"ham\"]\n",
    "print(f\"The shape of the spam_data is {spam_data.shape}. The shape of the ham_data is {ham_data.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T01:56:45.036145Z",
     "start_time": "2021-04-07T01:56:41.160986Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(data[\"text\"].values)\n",
    "\n",
    "# create the vocabulary list\n",
    "vocabulary = set(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate Likelihood Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T02:58:34.345684Z",
     "start_time": "2021-04-07T02:58:31.941643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the vocabulary list\n",
    "import spacy, string\n",
    "nlp = spacy.load('en_core_web_sm') # python3 -m spacy download en\n",
    "from nltk import word_tokenize\n",
    "\n",
    "likelihood_table = pd.DataFrame(columns=[\"spam\", \"ham\"], index=list(vocabulary)).fillna(0)\n",
    "\n",
    "# populate the spam column in our likelihood table\n",
    "for i, sentence in enumerate(spam_data[\"text\"].values):\n",
    "    for token in word_tokenize(sentence):\n",
    "        if token.lower() not in likelihood_table.index:\n",
    "            likelihood_table.loc[token.lower(), \"spam\"] = 1\n",
    "        else:\n",
    "            likelihood_table.loc[token.lower(), \"spam\"] += 1\n",
    "likelihood_table.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T02:58:47.301533Z",
     "start_time": "2021-04-07T02:58:37.393504Z"
    }
   },
   "outputs": [],
   "source": [
    "# populate the ham column in our likelihood table\n",
    "for i, sentence in enumerate(ham_data[\"text\"].values):\n",
    "    for token in nlp(sentence):\n",
    "        if token.text.lower() not in likelihood_table.index:\n",
    "            likelihood_table.loc[token.text.lower(), \"ham\"] = 1\n",
    "        else:\n",
    "            likelihood_table.loc[token.text.lower(), \"ham\"] += 1\n",
    "likelihood_table.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T02:58:51.577176Z",
     "start_time": "2021-04-07T02:58:51.555226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>wife</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>school</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ard</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paying</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           spam  ham\n",
       "wife        0.0  2.0\n",
       "school      0.0  5.0\n",
       "sentiment   0.0  0.0\n",
       "ard         0.0  6.0\n",
       "paying      0.0  3.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood with \"Fun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T02:58:55.933417Z",
     "start_time": "2021-04-07T02:58:55.918460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam    1.0\n",
       "ham     2.0\n",
       "Name: fun, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_table.loc[\"fun\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency with \"Won\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about words like `won`?\n",
    "```python\n",
    "likelihood_table.loc[\"won\"]\n",
    "```\n",
    "Output:\n",
    "```\n",
    "spam    16.0\n",
    "ham      0.0\n",
    "Name: won, dtype: float64\n",
    "```\n",
    "# Edge Cases\n",
    "What is $P(w = won|c = ham)$? If even one of the words' class-conditional probabilities is 0, then the entire likelihood will be zero, since the likelihood is simply the product of all the words' individual likelihoods.\n",
    "\n",
    "## Additive Smoothing Techniques\n",
    "\n",
    "We can define a new likelihood for the word `won`:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P_{new}(w = won | c = ham) = \\frac{N_{ham, won} + \\alpha}{N_{ham} + \\alpha d}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here, $N_{ham, won}$ is the number of times `won` appears in a text message that is classified as ham, and $N_{ham}$ is simply the total number of messages that are classified as ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Optimizations to Improve Naive Bayes Probabilistic Models for Text Classification\n",
    "\n",
    "- to may be useful to simply create a simple **co-occurence matrix**, and **run a correlation analysis** on the features (words). If certain words have extremely high correlations, you may wish to take them out, or fuse them into a single entity.\n",
    "- apply smoothing techniques to handle **out-of-vocabulary test words**\n",
    "- **ensemble techniques like bagging / boosting** do **not** help. There isn't any \"variation\" in a Naive Bayes model. Given the same trained corpus $C$, and a new text message $m$, a Naive Bayes model will always output the same prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V is the number of unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Words as Probabilities\n",
    "\n",
    "We can represent a sentence (a sequence of words) mathematically as \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "w = \\{{w_0, w_1, w_2, \\dots,w_{s-1}}\\}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here, **$s$** represents the total number of words in the sentence. **$w_{0}$** represents the first word in the sentence, **$w_{1}$** represents the second word in the sentence, and so on.\n",
    "\n",
    "# Exercise:\n",
    "\n",
    "`Older people, like everyone else, can benefit from accessing ride-sharing, but many are not comfortable with smart-phones.`\n",
    "\n",
    "You can ignore punctuation and capitalization for now.\n",
    "\n",
    "1. What is $s$? s=17\n",
    "2. What is $w_4$? What is $w_6$?\n",
    "3. What is $V$ (this corpus' vocabulary size, assuming this is the only sentence in the corpus)? You can do this the hard way, by counting manually.\n",
    "\n",
    "```python\n",
    "sentence = \"Older people, like everyone else, can benefit from accessing ride-sharing, but many are not comfortable with smart-phones\"\n",
    "\n",
    "import re # the most efficient, concise way (less readable)\n",
    "vocabulary = set([re.sub(r'[^\\w\\s]','',word).lower() for word in sentence.split()])\n",
    "print(\"The size of the vocabulary is {} words\".format(vocabulary))\n",
    "```\n",
    "\n",
    "# Independence\n",
    "\n",
    "In statistics, two events are independent if the outcome of one event does not affect the probability of the outcomes of another event.\n",
    "\n",
    "![https://en.wikipedia.org/wiki/Independence_(probability_theory)](images/prob_independence.svg)\n",
    "\n",
    "You will also often see this written as \n",
    "$$\n",
    "\\begin{equation}\n",
    "P(A,B) = P(A) * P(B)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In other words, an event A is independent of event B if the **probability of event A and event B happening together** is equal to **the probability of event A multplied by the probability of event B**.\n",
    "\n",
    "![https://en.wikipedia.org/wiki/Independence_(probability_theory)](images/conditional-probability.png)\n",
    "\n",
    "# Bigram Model\n",
    "\n",
    "A bigram is a group of two tokens (frequently words) that are treated as one distinct entity. For instance, the distinct bigrams in the sentence `I am home now` would be\n",
    "```python\n",
    "bigrams = [\n",
    "    (\"I\", \"am\"),\n",
    "    (\"am\", \"home\"),\n",
    "    (\"home\", \"now\")\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "### Exercise:\n",
    "Write a Python function to find all the bigrams in the sentence\n",
    "`In recent years, Johnson & Johnson has been focusing more on its high-margin pharmaceutical segment via acquisitions.`\n",
    "\n",
    "**Hints**:\n",
    "- split the sentence into a list of individual words (`my_sentence.split()`)\n",
    "- remove punctuation\n",
    "- lowercase all the letters\n",
    "- use a for loop to iterate through this list, getting the **i-th** and **i + 1-th** elements of the list\n",
    "\n",
    "**Challenge**:\n",
    "Generalize this function to work with `n-grams`.\n",
    "\n",
    "## Language Model\n",
    "\n",
    "Are words in a sentence conditionally independent from each other? In other words, does knowing that the first word `The` change your belief in the likelihood of the second word that follows?\n",
    "\n",
    "Which of the following sentences is more likely?\n",
    "\n",
    "```python\n",
    "sentence_A = \"Jack went to Wal-Mart.\"\n",
    "sentence_B = \"at and the be of I\"\n",
    "```\n",
    "Notice that all the words in sentence B come from [Wikipedia most common words in the English language](https://en.wikipedia.org/wiki/Most_common_words_in_English). Yet we intuitively know that the sentence is nonsensical and is unlikely to be seen in natural language.\n",
    "\n",
    "We can express the likelihood of a sentence $w$ as $p(w)$, and define it as\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(w) = \\prod_{i=0}^{s}p(w_{i+1}|w_{i})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "If we want to generalize this to an **N-Gram** model:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(w) = \\prod_{i=0}^{s}p(w_i |w_{i-n+1}, w_{i-n+2}, \\dots, w_{i})\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(w) = \\prod_{i=0}^{s}p(w_i | w_{i-n}^{i})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here, $w_{i-n}^{i} = w_{i-n+1}, w_{i-n+2}, \\dots, w_{i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_A = \"Jack went to Wal-Mart.\"\n",
    "\n",
    "'''\n",
    "p(w) = p(w_1=went | w_o=Jack) * p(w_2=to | w_1=went)...\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Eat dinner at home\",\n",
    "    \"He needs to go to the store\",\n",
    "    \"She needs to go home\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get a list of all unique tokens in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Calculate the transition frequencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calculate the transition probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Bigrams Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T03:02:52.194049Z",
     "start_time": "2021-04-07T03:02:50.845191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'wanted'), ('wanted', 'to'), ('to', 'grab'), ('grab', 'breakfast'), ('breakfast', 'one'), ('one', 'morning'), ('morning', 'before'), ('before', 'work'), ('work', 'since'), ('since', 'it')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "reviews_df = pd.read_csv(\"mcdonalds-yelp-negative-reviews.csv\", encoding=\"latin-1\")\n",
    "for review in reviews_df[\"review\"]:    \n",
    "    bigram = list(nltk.bigrams(word_tokenize(review)))\n",
    "print(bigram[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Bigrams Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T03:03:12.131808Z",
     "start_time": "2021-04-07T03:03:11.657827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1525, 64297)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Only get bigrams of 2\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(reviews_df[\"review\"])\n",
    "\n",
    "bigram_features = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "bigram_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T03:04:15.395498Z",
     "start_time": "2021-04-07T03:04:15.361545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00 am</th>\n",
       "      <th>00 for</th>\n",
       "      <th>00 in</th>\n",
       "      <th>00 meal</th>\n",
       "      <th>00 pm</th>\n",
       "      <th>00 sunday</th>\n",
       "      <th>000 mile</th>\n",
       "      <th>00am and</th>\n",
       "      <th>00am on</th>\n",
       "      <th>00am service</th>\n",
       "      <th>...</th>\n",
       "      <th>zip by</th>\n",
       "      <th>zombie apocalypse</th>\n",
       "      <th>zombie turned</th>\n",
       "      <th>zombie vampire</th>\n",
       "      <th>zombies anyway</th>\n",
       "      <th>zombies appeared</th>\n",
       "      <th>zombies on</th>\n",
       "      <th>zombies were</th>\n",
       "      <th>zoom up</th>\n",
       "      <th>î_ northside</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1525 rows × 64297 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00 am  00 for  00 in  00 meal  00 pm  00 sunday  000 mile  00am and  \\\n",
       "0         0       0      0        0      0          0         0         0   \n",
       "1         0       0      0        0      0          0         0         0   \n",
       "2         0       0      0        0      0          0         0         0   \n",
       "3         0       0      0        0      0          0         0         0   \n",
       "4         0       0      0        0      0          0         0         0   \n",
       "...     ...     ...    ...      ...    ...        ...       ...       ...   \n",
       "1520      0       0      0        0      0          0         0         0   \n",
       "1521      0       0      0        0      0          0         0         0   \n",
       "1522      0       0      0        0      0          0         0         0   \n",
       "1523      0       0      0        0      0          0         0         0   \n",
       "1524      0       0      0        0      0          0         0         0   \n",
       "\n",
       "      00am on  00am service  ...  zip by  zombie apocalypse  zombie turned  \\\n",
       "0           0             0  ...       0                  0              0   \n",
       "1           0             0  ...       0                  0              0   \n",
       "2           0             0  ...       0                  0              0   \n",
       "3           0             0  ...       0                  0              0   \n",
       "4           0             0  ...       0                  0              0   \n",
       "...       ...           ...  ...     ...                ...            ...   \n",
       "1520        0             0  ...       0                  0              0   \n",
       "1521        0             0  ...       0                  0              0   \n",
       "1522        0             0  ...       0                  0              0   \n",
       "1523        0             0  ...       0                  0              0   \n",
       "1524        0             0  ...       0                  0              0   \n",
       "\n",
       "      zombie vampire  zombies anyway  zombies appeared  zombies on  \\\n",
       "0                  0               0                 0           0   \n",
       "1                  0               0                 0           0   \n",
       "2                  0               0                 0           0   \n",
       "3                  0               0                 0           0   \n",
       "4                  0               0                 0           0   \n",
       "...              ...             ...               ...         ...   \n",
       "1520               0               0                 0           0   \n",
       "1521               0               0                 0           0   \n",
       "1522               0               0                 0           0   \n",
       "1523               0               0                 0           0   \n",
       "1524               0               0                 0           0   \n",
       "\n",
       "      zombies were  zoom up  î_ northside  \n",
       "0                0        0             0  \n",
       "1                0        0             0  \n",
       "2                0        0             0  \n",
       "3                0        0             0  \n",
       "4                0        0             0  \n",
       "...            ...      ...           ...  \n",
       "1520             0        0             0  \n",
       "1521             0        0             0  \n",
       "1522             0        0             0  \n",
       "1523             0        0             0  \n",
       "1524             0        0             0  \n",
       "\n",
       "[1525 rows x 64297 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T03:03:32.936473Z",
     "start_time": "2021-04-07T03:03:32.742760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1525, 8379)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(reviews_df[\"review\"])\n",
    "\n",
    "unigram_features = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "unigram_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model Evaluation: Choosing n in an n-Gram Model\n",
    "\n",
    "- the larger the dataset, and by implication, the more rich the corpus, the larger the n we can likely try.\n",
    "- in practice, $n = 2$, $n = 3$, $n = 4$ work well. A larger $n$ tends to begin to overfit (and may be computationally extremely expensive). Remember the **bias-variance** tradeoff:\n",
    "![http://scott.fortmann-roe.com/docs/BiasVariance.html](images/biasvariance.png)\n",
    "Here, as $n \\rightarrow \\infty$, model complexity increases dramatically.\n",
    "- **tune $n$ based on the performance of the downstream model**: usually n-gram models are the first step in a broader sentiment analysis prediction model, or topic modelling model, recommendation system, or sequence-to-sequence translation task.\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "Look again at the definition of likelihood for a particular sentence:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "L = p(w) = \\prod_{i=0}^{s}p(w_i | w_{i-n}^{i})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here, $w_{i-n}^{i}$ stands for $w_{i-n}, w_{i-n+1}, ..., w_{i-1}$ (if you have bi-gram (**`n=2`**) model, then you would have $w_{i-2}, w_{i-1}$)\n",
    "\n",
    "Is it reasonable to compare two sentences, one with $s=4$ (sentence length of 4 words) with the one below in Sentence B?\n",
    "\n",
    "##### Sentence A:\n",
    "> *I love to eat.*\n",
    "\n",
    "##### Sentence B:\n",
    "> *My escort was an exceptionally genial sixty-seven-year-old man named Don Seely, an electrical engineer who said that he was between jobs and using the unwanted free time to volunteer his services to the Northern Kentucky Tea Party, the rally’s host organization, as a Webmaster.*\n",
    "\n",
    "Answer: **No**. A common way of quantifying the likelihood of your n-gram models, accounting for different sizes of test corpuses, is to use **perplexity**. Remember that our likelihood of seeing a particular sentence is \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P = \\frac{1}{\\sqrt[N]{p(w)}}\n",
    "\\end{equation}\n",
    "$$\n",
    "$N$ is the length of all the words in the test sentence. We typically use perplexity, instead of simply likelihood, as the overall model evaluation metric, because in general, **in order to compare two different models**, they should be using the same test corpus / vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity (want low) takes into account the length of the sentence. I love to eat is more likely to occur because it is shorter.\n",
    "\n",
    "Smaller the perplexity, the more likely it is to occur in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T03:16:20.184640Z",
     "start_time": "2021-04-07T03:16:19.705571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Perplexity')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def perplexity(likelihood, N):\n",
    "    return 1 / (likelihood ** (1/N))\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for i in range(5, 500):\n",
    "    x.append(i)\n",
    "    y.append(perplexity(likelihood=.204, N=i)) # an example likelihood of .004\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"N\")\n",
    "plt.ylabel(\"Perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Out-of-Vocabulary Words\n",
    "\n",
    "Let's pretend that our training corpus is\n",
    "\n",
    "> *This is mistaken logic. It is true that a high variance and low bias model can preform well in some sense.*\n",
    "\n",
    "Our test corpus is\n",
    "\n",
    "> *This **is not** true.*\n",
    "\n",
    "Assuming a bi-gram model is used, what is the **perplexity** of our model? \n",
    "\n",
    "We don't actually need to count each bi-gram. **The answer is infinity**. Why?\n",
    "$\n",
    "\\begin{equation}\n",
    "p(w_i = not | w_{i-1} = is) = 0\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "What you can do instead:\n",
    "\n",
    "* Look at the **frequency distribution** of words in your corpus\n",
    "* Decide upon some **threshold cutoff**, where every word below that threshold frequency will be converted into an **`UNKNOWN`** token. Now, whenever a new word appears that is out of vocabulary, you simply convert it into **`UNKNOWN`** and run the tests as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix_word=['I','eat','RARE_TOKEN']\n",
    "\n",
    "'I eat RARE_TOKEN'\n",
    "# think imputation\n",
    "# has to buckets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
